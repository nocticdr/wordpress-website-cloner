# WordPress Website Cloner

A Python tool for creating static copies of WordPress websites with intelligent crawling and batch download capabilities.

## Features

- **WordPress Site Analysis**: Analyzes site structure and content via REST API
- **Multiple Cloning Modes**: 
  - Recent posts only
  - Homepage + key pages
  - Homepage + random posts
  - Full site clone
  - Custom URL specification
- **Batch Download Support**: Skip existing files to continue downloads incrementally
- **Asset Download**: Downloads CSS, JavaScript, and images with local path conversion
- **Respectful Crawling**: Configurable delays and limits

## How It Works

```mermaid
flowchart TD
    A[Start: Run Script] --> B[Enter WordPress URL]
    B --> C[Analyze Site Structure]
    C --> D[Check WordPress REST API]
    D --> E[Estimate Site Size]
    E --> F[Present Configuration Options]
    
    F --> G{Choose Clone Mode}
    G -->|Mode 1| H[Recent Posts Only]
    G -->|Mode 2| I[Homepage + Key Pages]
    G -->|Mode 3| I
    G -->|Mode 4| J[Full Site Clone]
    G -->|Mode 5| K[Custom URLs]
    
    H --> L[Get Recent Posts via API]
    I --> M[Breadth-First Crawling]
    J --> N[Get All Posts & Pages via API]
    K --> O[Process Custom URL List]
    
    L --> P[Check Existing Files]
    M --> P
    N --> P
    O --> P
    
    P --> Q{File Already Exists?}
    Q -->|Yes| R[Skip File]
    Q -->|No| S[Download Page]
    
    S --> T[Download Assets<br/>CSS, JS, Images]
    T --> U[Convert Links to Relative]
    U --> V[Save HTML File]
    
    R --> W{More Pages?}
    V --> W
    W -->|Yes| X[Wait Delay]
    X --> P
    W -->|No| Y[Generate Summary]
    
    Y --> Z[Complete: Static Site Ready]
    
    style A fill:#e1f5fe
    style Z fill:#c8e6c9
    style Q fill:#fff3e0
    style G fill:#f3e5f5
```

## Installation

1. Clone this repository
2. Create a virtual environment:
   ```bash
   python3 -m venv wordpress-cloner-env
   source wordpress-cloner-env/bin/activate  # On Windows: wordpress-cloner-env\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install requests beautifulsoup4 lxml
   ```

## Usage

```bash
python3 website_cloner.py https://example.com
```

The script will:
1. Present a main menu with options to count, clone, or both
2. Analyze the WordPress site structure (if counting)
3. Present interactive configuration options (if cloning)
4. Download and save pages with assets
5. Create a static copy in the `cloned_[domain]` directory

## Interactive UI Examples

### Main Menu
```
ğŸ” Complete WordPress Site Analyzer & Cloner
ğŸ¯ Target: https://example.com

============================================================
ğŸ¯ MAIN MENU
============================================================
What would you like to do?
   1. Count URLs only (quick analysis)
   2. Clone site only (skip counting)
   3. Both count and clone (recommended)
   4. Exit

   Enter choice (1/2/3/4, default 1): 1
   âœ… Selected: Count URLs only
```

### Site Analysis
```
ğŸ” Complete WordPress Site Analyzer & Cloner
ğŸ¯ Target: https://example.com

ğŸ” Analyzing https://example.com...
ğŸ—ºï¸  Checking sitemap hierarchy...
   âœ… Found top-level sitemap: https://example.com/sitemap_index.xml
   ğŸ“‹ This is a sitemap index with 4 sub-sitemaps:
      1. sitemap-posts.xml: 150 URLs
      2. sitemap-pages.xml: 25 URLs
      3. sitemap-categories.xml: 12 URLs
      4. sitemap-tags.xml: 45 URLs
   ğŸ“Š Total internal URLs collected: 232
   ğŸ’¾ Saved 232 URLs to cloned_example_com/all_sitemap_urls.txt
   ğŸ’¾ Saved 150 URLs to cloned_example_com/sitemap-posts.xml_urls.txt
   ğŸ’¾ Saved 25 URLs to cloned_example_com/sitemap-pages.xml_urls.txt

ğŸ“‹ SITE ANALYSIS (from sitemap):
   ğŸ“ Posts: 150
   ğŸ“„ Pages: 25
   ğŸ“‚ Categories: 12
   ğŸ·ï¸  Tags: 45
   ğŸ“Š Total URLs: 232
   ğŸŸ¡ MEDIUM SITE - Consider batching
   â„¹ï¸  Recommendation based on sitemap count (232 URLs)
```

### Smart Configuration (Small Site - â‰¤300 URLs)
```
============================================================
ğŸ› ï¸  CLONING CONFIGURATION
============================================================
Site has 35 posts and 4 pages
ğŸ“Š Total URLs available: 42

ğŸ” Checking 42 URLs from all_sitemap_urls.txt...
ğŸ“‹ Found 15 URLs that need to be downloaded:
    1. https://fingreen.org/
    2. https://fingreen.org/about/
    3. https://fingreen.org/resources/
    4. https://fingreen.org/news/
    5. https://fingreen.org/post-1/
   ... and 10 more

ğŸ“‹ MISSING URLS DETECTED:
   ğŸ“Š 15 URLs need to be downloaded
   ğŸ¯ These will be prioritized for cloning

âœ… SMALL SITE DETECTED (42 URLs)
   ğŸ“Š Total URLs is less than cut-off of 300
   ğŸš€ Will proceed with full clone
   â±ï¸  Estimated time: ~5 minutes
   âš™ï¸  Auto-configured settings:
      â€¢ Max pages: 300
      â€¢ Crawl depth: 10
      â€¢ Request delay: 1.0s
      â€¢ Auto-open browser: Yes
```

### Smart Skip (All URLs Already Exist)
```
============================================================
ğŸ› ï¸  CLONING CONFIGURATION
============================================================
Site has 35 posts and 4 pages
ğŸ“Š Total URLs available: 42
ğŸ“ Found 51 existing HTML files in cloned_fingreen_org
   These will be skipped to avoid re-downloading
ğŸ” Checking 42 URLs from all_sitemap_urls.txt...
âœ… All URLs from sitemap already exist as HTML files

âœ… ALL URLS ALREADY EXIST:
   ğŸ“Š All URLs from sitemap already have HTML files
   ğŸš€ Will skip to completion summary

âœ… SMALL SITE DETECTED (42 URLs)
   ğŸ“Š Total URLs is less than cut-off of 300
   ğŸš€ Will proceed with full clone
   â±ï¸  Estimated time: ~5 minutes
   âš™ï¸  Auto-configured settings:
      â€¢ Max pages: 300
      â€¢ Crawl depth: 10
      â€¢ Request delay: 1.0s
      â€¢ Auto-open browser: Yes

ğŸš€ Starting clone of https://fingreen.org
ğŸ“‹ Found 0 URLs to process

ğŸ”„ PROCESSING PAGES:

ğŸ‰ CLONING COMPLETED!
ğŸ“Š Pages processed: 0
ğŸ“ Output directory: cloned_fingreen_org

ğŸ“‹ BATCH SUMMARY:
   ğŸ“„ New pages downloaded: 0
   â­ï¸  Pages already existed: 51
   ğŸ“Š Total pages in directory: 51

ğŸŒ Opening cloned site...
ğŸŒ Opened cloned_fingreen_org/index.html in your default browser
```

### Manual Configuration (Large Site - >300 URLs)
```
============================================================
ğŸ› ï¸  CLONING CONFIGURATION
============================================================
Site has 150 posts and 25 pages
ğŸ“Š Total URLs available: 500

ğŸ“Š LARGE SITE DETECTED (500 URLs)
   â±ï¸  Time estimates:
      â€¢ 300 URLs: ~5 minutes
      â€¢ Every additional 60 URLs: +1 minute
      â€¢ Your site (500 URLs): ~8 minutes

1ï¸âƒ£ How many pages maximum do you want to clone?
   Current default: 50
   Recommended for testing: 50-100
   For full clone: 500
   Enter max pages (or press Enter for default): 100

2ï¸âƒ£ What content do you want to clone?
   1. Recent posts only (quick test)
   2. Homepage + key pages only
   3. Homepage + random sample of posts
   4. All posts and pages (full clone)
   5. Custom URLs (specify exact pages/paths)
   Enter choice (1/2/3/4/5): 2

3ï¸âƒ£ How deep should the crawler go?
   1 = Only direct links
   2 = Two levels deep (recommended)
   3 = Three levels deep
   10 = Maximum depth (for large sites)
   Enter depth (1-10, default 2): 2

4ï¸âƒ£ Delay between requests (be nice to the server)
   Current: 1 seconds
   Minimum: 1 second (recommended for large sites)
   Enter delay in seconds (or press Enter): 1.5

âœ… CONFIGURATION SUMMARY:
   ğŸ“Š Max pages: 100
   ğŸ“ Clone mode: minimal
   ğŸ” Crawl depth: 2
   â±ï¸  Request delay: 1.5s

   Proceed with these settings? (y/n): y
```

### Custom URL Input (Mode 5)
```
ğŸ“‹ CUSTOM URL INPUT
   You can specify URLs in several ways:
   1. Full URLs: https://www.example.com/about/
   2. Paths only: /about/ or about/
   3. Multiple URLs separated by commas
   4. One URL per line (press Enter twice when done)

   Examples:
   â€¢ /about/, /contact/, /products/
   â€¢ https://www.example.com/specific-post/
   â€¢ category/electronics/

   Choose input method (1=single line, 2=multiple lines): 1

   Enter URLs (separated by commas): /about/, /contact/, /products/, /blog/
   âœ… Added: https://example.com/about/
   âœ… Added: https://example.com/contact/
   âœ… Added: https://example.com/products/
   âœ… Added: https://example.com/blog/
   ğŸ  Added homepage: https://example.com

   ğŸ“Š Total custom URLs: 5
```

### Download Progress
```
ğŸš€ Starting clone of https://example.com

ğŸ“ Found 12 existing HTML files in cloned_example_com
   These will be skipped to avoid re-downloading

ğŸ—ºï¸  Using sitemap URLs (much faster than API)
âœ… Added homepage: https://example.com
âœ… Added 163 URLs from sitemap
â­ï¸  Skipped 12 URLs (already exist)

ğŸ“‹ Using custom URLs:
    1. âœ… https://example.com
    2. âœ… https://example.com/about/
    3. â­ï¸  SKIPPED (already exists): https://example.com/contact/
    4. âœ… https://example.com/products/
    5. âœ… https://example.com/blog/

ğŸ“Š Summary: 4 new URLs, 1 already exist

ğŸ“‹ PROCESSING ORDER:
    1. ğŸ  Homepage: https://example.com
    2. ğŸ“„ Custom Page: https://example.com/about/
    3. ğŸ“„ Custom Page: https://example.com/products/
    4. ğŸ“„ Custom Page: https://example.com/blog/

ğŸ”„ PROCESSING PAGES:

[ 1/ 4] ğŸŒ Processing: https://example.com
   ğŸ“¦ Downloading assets...
   ğŸ“ Downloaded asset: style.css
   ğŸ“ Downloaded asset: script.js
   ğŸ“ Downloaded asset: logo.png
   âœ… Saved: index.html

[ 2/ 4] ğŸŒ Processing: https://example.com/about/
   ğŸ“¦ Downloading assets...
   âœ… Saved: about_.html

[ 3/ 4] â­ï¸  SKIPPED (already exists): https://example.com/contact/ -> contact_.html

[ 4/ 4] ğŸŒ Processing: https://example.com/products/
   ğŸ“¦ Downloading assets...
   âœ… Saved: products_.html
```

### Completion Summary
```
ğŸ‰ CLONING COMPLETED!
ğŸ“Š Pages processed: 3
ğŸ“ Output directory: cloned_example_com
ğŸŒ Open cloned_example_com/index.html to view

ğŸ“‹ BATCH SUMMARY:
   ğŸ“„ New pages downloaded: 3
   â­ï¸  Pages already existed: 12
   ğŸ“Š Total pages in directory: 15

ğŸ“‹ CUSTOM URLS CLONED:
   ğŸ“„ Total custom pages: 3
   ğŸ  Homepage included: Yes
```

## Configuration Options

- **Max Pages**: Limit the number of pages to download
- **Clone Mode**: Choose from 5 different cloning strategies
- **Crawl Depth**: Control how deep the crawler goes (1-3 levels)
- **Request Delay**: Set delays between requests to be respectful to servers

## Batch Downloads

The tool supports incremental downloads:
- Automatically detects existing HTML files
- Skips already downloaded pages
- Perfect for large sites that need to be downloaded in batches

## Output Structure

```
cloned_example_com/
â”œâ”€â”€ index.html (homepage)
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ css/
â”‚   â”œâ”€â”€ js/
â”‚   â””â”€â”€ images/
â”œâ”€â”€ page1.html
â”œâ”€â”€ page2.html
â””â”€â”€ ...
```

## Requirements

- Python 3.7+
- requests
- beautifulsoup4
- lxml

## License

This project is open source. Use responsibly and respect website terms of service and robots.txt files.

## Disclaimer

This tool is for educational and legitimate backup purposes only. Always ensure you have permission to clone websites and respect copyright laws.
